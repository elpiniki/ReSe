<!doctype html>
<!--[if lt IE 7]> <html class="no-js lt-ie9 lt-ie8 lt-ie7" lang="en"> <![endif]-->
<!--[if IE 7]>    <html class="no-js lt-ie9 lt-ie8" lang="en"> <![endif]-->
<!--[if IE 8]>    <html class="no-js lt-ie9" lang="en"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
	<meta charset="utf-8">
	<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">

	<title>Research &lt; LCAP</title>
  <meta name="description" content="" />
<meta name="generator" content="GetSimple" />
<link rel="canonical" href="http://www.ece.jhu.edu/lcap/index.php?id=research" />
	
	<link rel="stylesheet" href="http://www.ece.jhu.edu/lcap/theme/twitter-bootstrap/css/bootstrap.css">
	<style>
	body {
	  padding-top: 50px;
	  padding-bottom: 40px;
	}
	</style>
	<link rel="stylesheet" href="http://www.ece.jhu.edu/lcap/theme/twitter-bootstrap/css/style.css">

	<script src="http://www.ece.jhu.edu/lcap/theme/twitter-bootstrap/js/libs/modernizr-2.5.3-respond-1.1.0.min.js"></script>
</head>
<body id="research">
<!--[if lt IE 7]><p class=chromeframe>Your browser is <em>ancient!</em> <a href="http://browsehappy.com/">Upgrade to a different browser</a> or <a href="http://www.google.com/chromeframe/?redirect=true">install Google Chrome Frame</a> to experience this site.</p><![endif]-->

  	<div class="navbar navbar-fixed-top navbar-inverse">
    <div class="navbar-inner">
	<div class="container">
	    <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
		<span class="icon-bar"></span>
	    </a>
	  <a class="brand" href="http://www.ece.jhu.edu/lcap/">
	    <img src="http://www.ece.jhu.edu/lcap/data/uploads/lcaplogo_small.png"></a>
	    <div class="nav-collapse">
		<ul class="nav">
		    <li class="mounya"><a href="http://www.ece.jhu.edu/lcap/index.php?id=mounya" title="Mounya">Dr. Elhilali</a></li>
<li class="people"><a href="http://www.ece.jhu.edu/lcap/index.php?id=people" title="People">People</a></li>
<li class="publications"><a href="http://www.ece.jhu.edu/lcap/index.php?id=publications" title="Publications">Publications</a></li>
<li class="current active  research"><a href="http://www.ece.jhu.edu/lcap/index.php?id=research" title="Research">Research</a></li>
<li class="teaching"><a href="http://www.ece.jhu.edu/lcap/index.php?id=teaching" title="Teaching">Teaching</a></li>
<li class="openings"><a href="http://www.ece.jhu.edu/lcap/index.php?id=openings" title="Openings">Openings</a></li>
<li class="contact"><a href="http://www.ece.jhu.edu/lcap/index.php?id=contact" title="Contact">Contact</a></li>
		    <!--li class="contact"><a href="http://ece.jhu.edu/~mounya/">Contact</a></li -->
		    		</ul>
	    </div><!--/.nav-collapse -->
	    <ul class="nav pull-right">
				    <li><a href="http://www.ece.jhu.edu/lcap/index.php?id=login" id="loginLink">Login</a></li>
			    </ul>
	</div>
    </div>
</div>
        
    <div class="container">
    
      <h2></h2>
    
	<div class="accordion" id="accordion2"><div class="accordion-group project"><div class="accordion-heading"><div class="accordion-toggle" data-toggle="collapse" href="#collapse0"><div class="title"><a >Sustained firing of central auditory neurons yields a discriminative spectro-temporal representation for natural sounds</a></div></div><div class="icons"> <a href="http://dx.doi.org/10.1371/journal.pcbi.1002982" title="Go to website"><img src="data/uploads/globe.png"></a>  <a href="data/uploads/pdfs/ploscompbio2013_carlin.pdf" title="View PDF"><img src="data/uploads/pdfico.png"></a>  <a href="getbib.php?pubId=1768" title="View Bibtex"><img src="data/uploads/bibico.png"></a></div></div><div id="collapse0" class="accordion-body collapse in"><div class="accordion-inner"><div class="image"><img src="data/uploads/projects/ploscompbio2013_carlin.png"></div><div class="abstract">The processing characteristics of neurons in the central auditory system are directly shaped by and reflect the statistics of  natural acoustic environments, but the principles that govern the relationship between natural sound ensembles and  observed responses in neurophysiological studies remain unclear. In particular, accumulating evidence suggests the  presence of a code based on sustained neural firing rates, where central auditory neurons exhibit strong, persistent  responses to their preferred stimuli. Such a strategy can indicate the presence of ongoing sounds, is involved in parsing  complex auditory scenes, and may play a role in matching neural dynamics to varying time scales in acoustic signals. In this  paper, we describe a computational framework for exploring the influence of a code based on sustained firing rates on the  shape of the spectro-temporal receptive field (STRF), a linear kernel that maps a spectro-temporal acoustic stimulus to the  instantaneous firing rate of a central auditory neuron. We demonstrate the emergence of richly structured STRFs that  capture the structure of natural sounds over a wide range of timescales, and show how the emergent ensembles resemble  those commonly reported in physiological studies. Furthermore, we compare ensembles that optimize a sustained firing  code with one that optimizes a sparse code, another widely considered coding strategy, and suggest how the resulting  population responses are not mutually exclusive. Finally, we demonstrate how the emergent ensembles contour the highenergy  spectro-temporal modulations of natural sounds, forming a discriminative representation that captures the full  range of modulation statistics that characterize natural sound ensembles. These findings have direct implications for our  understanding of how sensory systems encode the informative components of natural stimuli and potentially facilitate  multi-sensory integration.</div></div></div></div><div class="accordion-group project"><div class="accordion-heading"><div class="accordion-toggle" data-toggle="collapse" href="#collapse1"><div class="title"><a >A Multistream Feature Framework Based on Bandpass Modulation Filtering for Robust Speech Recognition</a></div></div><div class="icons"> <a href="http://dx.doi.org/10.1109/TASL.2012.2219526" title="Go to website"><img src="data/uploads/globe.png"></a>  <a href="data/uploads/pdfs/ieeetrans2013_nemala.pdf" title="View PDF"><img src="data/uploads/pdfico.png"></a>  <a href="getbib.php?pubId=1652" title="View Bibtex"><img src="data/uploads/bibico.png"></a></div></div><div id="collapse1" class="accordion-body collapse"><div class="accordion-inner"><div class="image"><img src="data/uploads/projects/ieeetrans2013_nemala.png"></div><div class="abstract">There is strong neurophysiological evidence suggesting that processing of speech signals in the brain happens along parallel paths which encode complementary information in the signal. These parallel streams are organized around a duality of slow vs. fast: Coarse signal dynamics appear to be processed separately from rapidly changing modulations both in the spectral and temporal dimensions. We adapt such duality in a multistream framework for robust speaker-independent phoneme recognition. The scheme presented here centers around a multi-path bandpass modulation analysis of speech sounds with each stream covering an entire range of temporal and spectral modulations. By performing bandpass operations along the spectral and temporal dimensions, the proposed scheme avoids the classic feature explosion problem of previous multistream approaches while maintaining the advantage of parallelism and localized feature analysis. The proposed architecture results in substantial improvements over standard and state-of-the-art feature schemes for phoneme recognition, particularly in presence of nonstationary noise, reverberation and channel distortions.</div></div></div></div><div class="accordion-group project"><div class="accordion-heading"><div class="accordion-toggle" data-toggle="collapse" href="#collapse2"><div class="title"><a >A multiresolution analysis for detection of abnormal lung sounds</a></div></div><div class="icons"> <a href="http://dx.doi.org/10.1109/EMBC.2012.6346630" title="Go to website"><img src="data/uploads/globe.png"></a>  <a href="data/uploads/pdfs/embs2012_emmanouilidou.pdf" title="View PDF"><img src="data/uploads/pdfico.png"></a>  <a href="getbib.php?pubId=1558" title="View Bibtex"><img src="data/uploads/bibico.png"></a></div></div><div id="collapse2" class="accordion-body collapse"><div class="accordion-inner"><div class="image"><img src="data/uploads/projects/embs2012_emmanouilidou.png"></div><div class="abstract">Automated analysis and detection of abnormal lung sound patterns has great potential for improving access to standardized diagnosis of pulmonary diseases, especially in low-resource settings. In the current study, we develop signal processing tools for analysis of paediatric auscultations recorded under non-ideal noisy conditions. The proposed model is based on a biomimetic multi-resolution analysis of the spectro-temporal modulation details in lung sounds. The methodology provides a detailed description of joint spectral and temporal variations in the signal and proves to be more robust than frequency-based techniques in distinguishing crackles and wheezes from normal breathing sounds.</div></div></div></div><div class="accordion-group project"><div class="accordion-heading"><div class="accordion-toggle" data-toggle="collapse" href="#collapse3"><div class="title"><a >Recognizing the message and the messenger: Biomimetic  Spectral Analysis for Robust Speech and Speaker Recognition</a></div></div><div class="icons"> <a href="http://dx.doi.org/10.1007/s10772-012-9184-y" title="Go to website"><img src="data/uploads/globe.png"></a>  <a href="data/uploads/pdfs/intjspeech2012_nemala.pdf" title="View PDF"><img src="data/uploads/pdfico.png"></a>  <a href="getbib.php?pubId=1569" title="View Bibtex"><img src="data/uploads/bibico.png"></a></div></div><div id="collapse3" class="accordion-body collapse"><div class="accordion-inner"><div class="image"><img src="data/uploads/projects/intjspeech2012_nemala.png"></div><div class="abstract">Humans are quite adept at communicating in presence of noise. However  most speech processing systems, like automatic speech and speaker  recognition systems, suffer from a significant drop in performance when  speech signals are corrupted with unseen background distortions. The  proposed work explores the use of a biologically-motivated  multi-resolution spectral analysis for speech representation. This  approach focuses on the information-rich spectral attributes of speech  and presents an intricate yet computationally-efficient analysis of the  speech signal by careful choice of model parameters. Further, the  approach takes advantage of an information-theoretic analysis of the  message and speaker dominant regions in the speech signal, and defines  feature representations to address two diverse tasks such as speech and  speaker recognition. The proposed analysis surpasses the standard  Mel-Frequency Cepstral Coefficients (MFCC), and its enhanced variants  (via mean subtraction, variance normalization and time sequence  filtering) and yields significant improvements over a state-of-the-art  noise robust feature scheme, on both speech and speaker recognition  tasks.</div></div></div></div><div class="accordion-group project"><div class="accordion-heading"><div class="accordion-toggle" data-toggle="collapse" href="#collapse4"><div class="title"><a >Biomimetic multi-resolution analysis for robust speaker recognition</a></div></div><div class="icons"> <a href="http://dx.doi.org/10.1186/1687-4722-2012-22" title="Go to website"><img src="data/uploads/globe.png"></a>  <a href="data/uploads/pdfs/eurasip2012_nemala.pdf" title="View PDF"><img src="data/uploads/pdfico.png"></a>  <a href="getbib.php?pubId=914" title="View Bibtex"><img src="data/uploads/bibico.png"></a></div></div><div id="collapse4" class="accordion-body collapse"><div class="accordion-inner"><div class="image"><img src="data/uploads/projects/eurasip2012_nemala.png"></div><div class="abstract">Humans exhibit a remarkable ability to reliably classify sound sources in the environment even in presence of high  levels of noise. In contrast, most engineering systems suffer a drastic drop in performance when speech signals are  corrupted with channel or background distortions. Our brains are equipped with elaborate machinery for speech  analysis and feature extraction, which hold great lessons for improving the performance of automatic speech  processing systems under adverse conditions. The work presented here explores a biologically-motivated  multi-resolution speaker information representation obtained by performing an intricate yet computationally-efficient  analysis of the information-rich spectro-temporal attributes of the speech signal. We evaluate the proposed features  in a speaker verification task performed on NIST SRE 2010 data. The biomimetic approach yields significant robustness  in presence of non-stationary noise and reverberation, offering a new framework for deriving reliable features for  speaker recognition and speech processing.</div></div></div></div><div class="accordion-group project"><div class="accordion-heading"><div class="accordion-toggle" data-toggle="collapse" href="#collapse5"><div class="title"><a >Music in our ears: the biological bases of musical timbre perception</a></div></div><div class="icons"> <a href="http://dx.doi.org/10.1371/journal.pcbi.1002759" title="Go to website"><img src="data/uploads/globe.png"></a>  <a href="data/uploads/pdfs/ploscompbio2012_patil.pdf" title="View PDF"><img src="data/uploads/pdfico.png"></a>  <a href="getbib.php?pubId=1552" title="View Bibtex"><img src="data/uploads/bibico.png"></a></div></div><div id="collapse5" class="accordion-body collapse"><div class="accordion-inner"><div class="image"><img src="data/uploads/projects/ploscompbio2012_patil.png"></div><div class="abstract">Timbre is the attribute of sound that allows humans and other animals to distinguish among different sound sources. Studies based on psychophysical judgments of musical timbre, ecological analyses of sound&#39;s physical characteristics as well as machine learning approaches have all suggested that timbre is a multifaceted attribute that invokes both spectral and temporal sound features. Here, we explored the neural underpinnings of musical timbre. We used a neuro-computational framework based on spectro-temporal receptive fields, recorded from over a thousand neurons in the mammalian primary auditory cortex as well as from simulated cortical neurons, augmented with a nonlinear classifier. The model was able to perform robust instrument classification irrespective of pitch and playing style, with an accuracy of 98.7%. Using the same front end, the model was also able to reproduce perceptual distance judgments between timbres as perceived by human listeners. The study demonstrates that joint spectro-temporal features, such as those observed in the mammalian primary auditory cortex, are critical to provide the rich-enough representation necessary to account for perceptual judgments of timbre by human listeners, as well as recognition of musical instruments.</div></div></div></div><div class="accordion-group project"><div class="accordion-heading"><div class="accordion-toggle" data-toggle="collapse" href="#collapse6"><div class="title"><a >Temporal coherence in the perceptual organization and cortical representation of auditory scenes</a></div></div><div class="icons"> <a href="http://dx.doi.org/10.1016/j.neuron.2008.12.005" title="Go to website"><img src="data/uploads/globe.png"></a>  <a href="data/uploads/pdfs/neuron2009_elhilali.pdf" title="View PDF"><img src="data/uploads/pdfico.png"></a>  <a href="getbib.php?pubId=437" title="View Bibtex"><img src="data/uploads/bibico.png"></a></div></div><div id="collapse6" class="accordion-body collapse"><div class="accordion-inner"><div class="image"><img src="data/uploads/projects/neuron2009_elhilali.png"></div><div class="abstract">Just as the visual system parses complex scenes into identifiable objects, the auditory system must organize sound elements scattered in frequency and time into coherent &quot;streams.&quot; Current neurocomputational theories of auditory streaming rely on tonotopic organization of the auditory system to explain the observation that sequential spectrally distant sound elements tend to form separate perceptual streams. Here, we show that spectral components that are well separated in frequency are no longer heard as separate streams if presented synchronously rather than consecutively. In contrast, responses from neurons in primary auditory cortex of ferrets show that both synchronous and asynchronous tone sequences produce comparably segregated responses along the tonotopic axis. The results argue against tonotopic separation per se as a neural correlate of stream segregation. Instead we propose a computational model of stream segregation that can account for the data by using temporal coherence as the primary criterion for predicting stream formation.</div></div></div></div><div class="accordion-group project"><div class="accordion-heading"><div class="accordion-toggle" data-toggle="collapse" href="#collapse7"><div class="title"><a >Interaction between attention and bottom-up saliency mediates the representation of foreground and background in an auditory scene</a></div></div><div class="icons"> <a href="http://dx.doi.org/10.1371/journal.pbio.1000129" title="Go to website"><img src="data/uploads/globe.png"></a>  <a href="data/uploads/pdfs/plosbiology2009_elhilali.pdf" title="View PDF"><img src="data/uploads/pdfico.png"></a>  <a href="getbib.php?pubId=446" title="View Bibtex"><img src="data/uploads/bibico.png"></a></div></div><div id="collapse7" class="accordion-body collapse"><div class="accordion-inner"><div class="image"><img src="data/uploads/projects/plosbiology2009_elhilali.png"></div><div class="abstract">The mechanism by which a complex auditory scene is parsed into coherent objects depends on poorly understood interactions between task-driven and stimulus-driven attentional processes. We illuminate these interactions in a simultaneous behavioral-neurophysiological study in which we manipulate participants&#39; attention to different features of an auditory scene (with a regular target embedded in an irregular background). Our experimental results reveal that attention to the target, rather than to the background, correlates with a sustained (steady-state) increase in the measured neural target representation over the entire stimulus sequence, beyond auditory attention&#39;s well-known transient effects on onset responses. This enhancement, in both power and phase coherence, occurs exclusively at the frequency of the target rhythm, and is only revealed when contrasting two attentional states that direct participants&#39; focus to different features of the acoustic stimulus. The enhancement originates in auditory cortex and covaries with both behavioral task and the bottom-up saliency of the target. Furthermore, the target&#39;s perceptual detectability improves over time, correlating strongly, within participants, with the target representation&#39;s neural buildup. These results have substantial implications for models of foreground/background organization, supporting a role of neuronal temporal synchrony in mediating auditory object formation.</div></div></div></div></div><div class="funding">
<h4>This research is funded by:</h4>
<ul class="thumbnails"><li class="span1">
    <a href="http://www.wpafb.af.mil/afrl/afosr/" class="thumbnail">
      <table><tr><td>
      <img src="data/uploads/funding/afosr.jpg"></td></tr></table>
  </a></li><li class="span1">
    <a href="http://www.gatesfoundation.org/" class="thumbnail">
      <table><tr><td>
      <img src="data/uploads/funding/billgates.jpg"></td></tr></table>
    </a></li><li class="span1">
    <a href="http://www.iarpa.gov/" class="thumbnail">
      <table><tr><td>
      <img src="data/uploads/funding/iarpa.jpg"></td></tr></table>
    </a></li><li class="span1">
    <a href="http://www.nasa.gov/" class="thumbnail">
      <table><tr><td>
      <img src="data/uploads/funding/nasa.jpg"></td></tr></table>
    </a></li><li class="span1">
    <a href="http://www.nih.gov/" class="thumbnail">
      <table><tr><td>
      <img src="data/uploads/funding/nih.jpg"></td></tr></table>
    </a></li><li class="span1">
    <a href="http://www.nsf.gov/" class="thumbnail">
      <table><tr><td>
      <img src="data/uploads/funding/nsf.jpg"></td></tr></table>
    </a></li><li class="span1">
    <a href="http://www.onr.navy.mil/" class="thumbnail">
      <table><tr><td>
      <img src="data/uploads/funding/onr.jpg"></td></tr></table>
    </a></li>
</ul>
</div>    
    <footer>
    <hr>
    Copyright © 2008-2013 Laboratory for Computational Audio Perception
    </footer>
    </div> <!-- /container -->
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min.js"></script>
<script src="http://www.ece.jhu.edu/lcap/theme/twitter-bootstrap/js/bootstrap.min.js"></script>
</body>
</html>
