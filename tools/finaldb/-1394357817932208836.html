<html><!-- slap your title below --><head><title>	Mohan's Web Page</title></head><FONT FACE=Helvetica,Arial><BASEFONT FACE=Helvetica,Arial><body bgcolor=#ffffff link=#1178c6 vlink=#1178c6 alink=#1178c6 marginheight=0 marginwidth=0><center><table width=100% border=0 cellspacing=10 cellpadding=0>	<tr>	<td bgcolor=#ffffff><!!--BEGIN WEBPAGE-->	<center>	<table width=100% border=0 cellspacing=1 cellpadding=2>		<tr>		<td bgcolor=#000000>			<table width=100% border=0 cellspacing=0 cellpadding=10>				<tr>				<td bgcolor=#b79e7b>						<center>						<font size=+3>						<b>Stochastic Estimation and Autonomous Robotics Lab</b>						<font size=+0>						<!--						<br>						<b></b> -->						</center>				</tr>				</td>			</table>		</tr>		</td>	</table>	</center>	<br>	<table width=80% border=0 align=right cellspacing=1 cellpadding=2>		<tr>		<td bgcolor=#000000>			<table width=100% border=0 cellspacing=0 cellpadding=10 cols=2>				<tr>				<td bgcolor=#ffffff>					<font size=+1>					<center><b>Objectives</b></center>					<br>					<font size=-1>					<div align="justify">					Our primary research interests include knowledge representation and reasoning, machine learning, computer vision and cognitive science as applied to autonomous mobile robots. We seek to develop algorithms that enable robots to collaborate with non-expert human participants, acquiring and using sensor inputs and high-level human feedback based on need and availability. Furthermore, we are interested in developing learning and inference algorithms for application domains characterized by a significant amount of uncertainty.					<br><br>					<a href="#Lab members"><b>Lab members (humans and robots!)</b></a><br>					<a href="#Collaborators"><b>Collaborators</b></a><br>					<a href="#Robotics projects"><b>Robotics projects</b></a>, <a href="#Other videos"><b>Other cool videos</b></a><br>					<a href="#ML projects"><b>Non-robotics projects</b></a><br><br>					<hr>					<a name="Lab members"></a>					<b>Lab director:</b> <a href="http://www.cs.ttu.edu/~smohan/">Dr. Mohan Sridharan</a><br><br>					<b>Student group:</b> <a href="./Images/LabPics/AllStudents_Summer2013a.JPG">Summer 2013</a> (including REU students). <br><br>					<b>Current Students:</b>						<ul>						<li> Batbold Myagmarjav. PhD (<a href="#Multimodal">Multimodal learning</a>) </li>						<li> Daniel Holman. PhD (<a href="#AgriApps">Agricultural applications</a>) </li>						<li> Jie Zhao. PhD (<a href="#Plan">Representation for visual learning</a>) </li>						<li> Han Xu. MSCS (<a href="#Plan">Representation for verbal reasoning</a>) </li>						</ul>					<b>Past (Graduate) Students:</b>						<ul>						<li> Sarah Rainge: <b>MSCS, May 2014</b> (<a href="./Images/LabPics/Sarah_MSDefense.JPG">thesis defense picture</a>) <br>						(<a href="./Papers/sRainge_thesis14.pdf">pdf</a>) </li>						<li> Shiqi Zhang: <b>PhD, August 2013</b> (<a href="./Images/LabPics/Shiqi_PhDDefense.JPG">dissertation defense picture</a>) <br>						(<a href="./Papers/sZhang_dissertation13.pdf">pdf</a>) </li>						<li> Xiang Li: <b>PhD, August 2013</b> (<a href="./Images/LabPics/Xiang_PhDDefense.JPG">dissertation defense picture</a>) <br>						(<a href="./Papers/xLi_dissertation13.pdf">pdf</a>)			 </li>						<li> Kimia Salmani: <b>MSCS, August 2013</b> (<a href="./Images/LabPics/Kimia_MSDefense.JPG">thesis defense picture</a>) <br>						(<a href="./Papers/kSalmani_thesis13.pdf">pdf</a>) </li>						<li> Justin Griggs: <b>MSEE, August 2012</b>, Co-supervised with Dr. Richard Gale <br>						(<a href="./Papers/jGriggs_thesis12.pdf">pdf</a>)   </li>						<li> Aaron Lee: <b>MSEE, December 2011</b>, Co-supervised with Dr. Richard Gale<br>						(<a href="./Papers/aLee_thesis11.pdf">pdf</a>) </li>						<li> Mamatha Aerolla: <b>MSCS, May 2011</b> <br>						(<a href="./Papers/mAerolla_thesis11.pdf">pdf</a>) </li>						</ul>					<b>Past (Undergraduate) Students:</b>						<ul>						<li> Patricia Andrews. B.S. (Colorado College), <u>REU Student</u>, <b>Summer 2013</b>. </li>						<li> Olatide Omojaro. B.S. (Georgia Perimeter College), <u>REU Student</u>, <b>Summer 2013</b>. </li>						<li> Aaron Hester. B.S. (Mathematics+CS), <u>REU Student</u>, <b>Summer 2013</b>. </li>						<li> Emilie Featherston. B.S. (co-supervised with Drs. Susan and Joseph Urban), <u>REU Student</u>, <b>Summer 2013</b>. </li>						<li> Christian Washington. B.S. (Louisiana State University), <u>REU Student</u>, <b>Summer 2012</b>.						<li> Catie Meador. B.S. (Swarthmore College), <u>REU Student</u>, <b>Summer 2012</b>.						<li> Sabyne Peeler. B.S. (Florida A&M University) co-supervised with Drs. Susan and Joseph Urban, <u>REU Student</u>, <b>Summer 2012</b>.						<li> Shiloh Huff. B.S. (co-supervised with Drs. Susan and Joseph Urban), <u>REU Student</u>, <b>Summer 2012</b>.						<li> Stephanie Graham. B.S. (co-supervised with Drs. Susan and Joseph Urban), <u>REU Student</u>, <b>Summer 2012</b>.						<li> Austin Ray. B.S. (co-supervised with Drs. Susan and Joseph Urban), <b>Spring 2012</b>.						<li> David South. B.S. (co-supervised with Drs. Susan and Joseph Urban), <b>Spring 2012</b>.						<li> Kevin Thomas. B.S. (co-supervised with Drs. Susan and Joseph Urban), <b>Spring 2012</b>.						<li> Jesse Kawell. B.S. (Samford University), <u>REU Student</u>, <b>Summer 2011</b>.						<li> David Kari. B.S. (California Baptist University), <u>REU Student</u>, <b>Summer 2011</b>.						<li> David Seibert. B.S. (Emory University), <u>REU Student</u>, <b>Summer 2011</b>.						<li> James Smith. B.S. (The University of Texas at Austin), <u>REU Student</u>, <b>Summer 2011</b>.						<li> Mary Shuman, B.S. (University of North Carolina at Charlotte, co-supervised with Dr. Susan Urban), <u>REU Student</u>, <b>Summer 2011</b>.						<li> Matthew Sullivan. B.S. (Computer Engineering), <b>Spring-Summer 2011</b>.						<li> Kshira Nadarajan. B.S. (Iowa State University), <b>Summer 2010</b>.						</ul>					Students interested in working with me should <a href="./Note2Collab.html">read this</a> before you contact me.	Some <b>robot platforms</b> used in experimental trials are shown below.<br> <br>					<a href="Images/RobotLearnAdapt/assistrobot.jpg"><img src="Images/RobotLearnAdapt/Small/assistrobot2.jpg"alt="Socially Assistive Robot"/></a>					<a href="Images/RobotLearnAdapt/videreNao.jpg"><img src="Images/RobotLearnAdapt/Small/videreNao2.jpg"alt="Nao and Erratic platforms"/></a>					<a href="Images/RobotLearnAdapt/allRobots1.jpg"><img src="Images/RobotLearnAdapt/Small/allRobots2.jpg"alt="All robot platforms"/></a> <br>					<a href="Images/RobotLearnAdapt/uavVersion1.jpg"><img src="Images/RobotLearnAdapt/Small/uavVersion1.jpg"alt="UAV version1"/></a>					<a href="Images/RobotLearnAdapt/uavVersion3.jpg"><img src="Images/RobotLearnAdapt/Small/uavVersion3.jpg"alt="UAV version3"/></a>					<br><br>					<hr/>					<h3><a name="Collaborators"></a><b>Current (and recent) collaborators:</b></h3> Some of our collaborators at TTU and elsewhere are listed below.					<ul>					<li> <a href="http://www.cs.utexas.edu/~pstone">Dr. Peter Stone</a>, UT-Austin (Computer Science, <b>Robotics</b>).</li>					<li> <a href="http://www.depts.ttu.edu/psy/people/kjones.php">Dr. Keith Jones</a>, TTU (Psychology, <b>HRI & Human factors</b>).</li>					<li> <a href="http://www.depts.ttu.edu/ieweb/faculty/main/patterson.php">Dr. Patrick Patterson</a>, TTU (Industrial Engineering, <b>Social Exchanges</b>).</li>					<li> <a href="http://www.cs.ttu.edu/~suurban">Dr. Susan Urban</a>, TTU (Industrial Engineering, <b>Adaptive event processing</b>). </li>					<li> <a href="http://www.ars.usda.gov/pandp/people/people.htm?personid=37079">Dr. Prasanna Gowda</a>, USDA-ARS (Soil & water management, <b>Agricultural irrigation management</b>).</li>					<li> <a href="http://www.uiw.edu/math/mccarron.html">Dr. Craig Mccarron</a>, University of the Incarnate Word (Mathematics, <b>Virtual students</b>).</li>					<li> <a href="http://webpages.acs.ttu.edu/hsarisar">Dr. Hamed Sari-Sarraf</a>, TTU (Electrical Engineering, <b>Pattern recognition</b>).</li>					<li> <a href="http://www.depts.ttu.edu/ece/faculty/gale/">Dr. Richard Gale</a>, <a href="http://www.depts.ttu.edu/ece/faculty/karp/"> Dr. Tanja Karp</a>, TTU (Electrical Engineering, <b>Robotics & engineering education</b>).</li>					<li> <a href="http://www.katharinehayhoe.com">Dr. Katharine Hayhoe</a>, TTU (Political Science/Climate Science Center, <b>Climate modeling</b>).</li>					</ul>					<hr/>					<h3><a name="Robotics projects"></a><b>Robotics Projects:</b></h3>					In the context of human-robot collaboration, we seek to answer the following key questions:					<ol>					<li><i>How to best enable robots to represent and reason with incomplete domain knowledge, incrementally revising the knowledge using information learned from sensors and high-level human feedback? </i></li>					<li><i>How to best enable robots to adapt these representation and reasoning capabilities for learning from multimodal sensor inputs and limited feedback from non-expert human participants?</i> </li>					</ol>					Although many sophisticated algorithms have been developed for the associated learning, adaptation and collaboration challenges, the integration of these challenges poses open problems even as it presents novel opportunities to address the individual challenges. We therefore seek to develop an integrated architecture that jointly addresses the learning, adaptation and collaboration challenges by exploiting their mutual dependencies. <br><br>					<b>Representative publications:</b><br>					<a href="http://www.cs.ttu.edu/~smohan">Mohan Sridharan</a>. <b>Integrating Visual Learning and Hierarchical Planning for Autonomy in Human-Robot Collaboration</b> In the AAAI Spring Symposium on <a href="http://people.csail.mit.edu/gdk/dir2/">Designing Intelligent Robots: Reintegrating AI II</a>, Stanford, USA, March 25-27, 2013. (<a href="./Papers/aaai13_integrate.pdf">pdf</a>) <br> <br>					<a href="http://www.cs.ttu.edu/~smohan">Mohan Sridharan</a>. <b>An Integrated Framework for Robust Human-Robot Interaction.</b> In Jose Garcia-Rodriguez and Miguel Cazorla (editors), <i>Robotic Vision: Technologies for Machine Learning and Vision Applications</i>, pages 281-301 (535), IGI Global, 2013 (web: December 28, 2012). (<a href="./Papers/bookchap12_HRIFramework.pdf">pre-publication pdf</a>) (<a href="http://www.igi-global.com/book/robotic-vision-technologies-machine-learning/69214">book website</a>) <br><br>					<hr width="70%">					The individual components of the architecture have developed into the research projects described below.					<ul>						<li><a name="Plan"></a><b style="color:red"><u>Knowledge Representation and Reasoning:</u></b> The objective is to exploit the complementary strengths of declarative programming and probabilistic graphical models to address the knowledge representation and reasoning challenges in robotics. Towards this objective, we integrate the commonsense reasoning capabilities of Answer Set Programming (ASP), a declarative language, with the probabilistic uncertainty modeling capabilities of hierarchical partially observable Markov decision processes (POMDPs). Robots use this architecture to represent and reason with qualitative and quantitative descriptions of knowledge and uncertainty obtained from sensor inputs and high-level human feedback. The image below is an overview of the architecture, while the videos illustrate the use of the architecture to localize target objects in indoor domains (<a href="./Images/RobotLearnAdapt/mapLocalize.jpg">learned map</a>). All algorithms are implemented in the Robot Operating System (ROS) framework.<br><br>						<center>						<a href="./Images/RobotLearnAdapt/aspPomdp.jpg"><img src="./Images/RobotLearnAdapt/Small/aspPomdp.jpg"alt="ASP+POMDP for KRR"/></a>&nbsp;&nbsp;&nbsp;&nbsp;						<a href="./Movies/Planning/aspPomdp_June2013a.mp4"><img src="./Images/RobotLearnAdapt/Small/aspPomdpPlan.jpg"alt="KRR for robots"/></a> </center>						<br> <br>						Some recent videos of experimental trials can be found on youtube: <a href="http://youtu.be/EvY_Jt-5BqM">video-1</a>, <a href="http://youtu.be/DqsR2qDayGQ">video-2</a>.						<!--						You can also look at a <b><a href="./Movies/Planning/mapLocalize_Sep2011.ogv">video<a></b> (<a href="./Movies/Planning/mapLocalize_Sep2011.flv"><b>compressed</b></a>) of one of the initial attempts at autonomous navigation between two points---the robot had some trouble convincing itself that it has reached the desired location! :-)						-->						<br><br>						<b>Representative publications:</b><br>						<a href="http://www.cs.ttu.edu/~shiqizha">Shiqi Zhang</a>, <a href="http://www.cs.ttu.edu/~smohan/">Mohan Sridharan</a> and Christian Washington. <b>Active Visual Planning for Mobile Robot Teams using Hierarchical POMDPs</b>. In the  <i>IEEE Transactions on Robotics (T-RO)</i>, Volume 29, Issue 4, 2013. (<a href="./Papers/tro13_activeVisualPlan.pdf">pdf</a>)						<br><br>						<a href="http://www.cs.ttu.edu/~shiqizha">Shiqi Zhang</a> and <a href="http://www.cs.ttu.edu/~smohan/">Mohan Sridharan</a>. <b>Integrating Declarative Programming and Probabilistic Planning on Robots.</b> In the AAAI Fall Symposium on <i>How Should Intelligence be Abstracted in AI Research: MDPs, Symbolic Representations, Artificial Neural Networks, or ____?</i>,  Arlington, USA, November 15-17, 2013. (<a href="./Papers/aaai13_probplanDeclarative.pdf">pdf</a>)						</li><br><br>						<hr width="70%"\><br><br>					    <li><a name="Vision"></a><b style="color:red"><u>Autonomous (visual) learning of object models:</u></b> The objective is to enable mobile robots to autonomously learn object models using local, global, temporal and contextual visual cues. Learning is triggered by motion cues, and object models consist of probabilistic representations of visual features with complementary properties. A pictorial representation of the object model is provided below:<br><br>						<center><a href="Images/RobotLearnAdapt/layeredObjModel.jpg"><img src="Images/RobotLearnAdapt/Small/layeredObjModel.jpg"alt="Object model structure"/></a>						<!--						<a href="Images/RobotLearnAdapt/objLearnExample.jpg"><img src="Images/RobotLearnAdapt/Small/objLearnExample.jpg"alt="Example of learned model"/></a> -->						</center>						<br> <br>						The learned models can be used for object recognition in complex scenes. You can look at a <b><a href="http://www.youtube.com/watch?v=2HPsW3w6nyw">video</a></b> of the learning and recognition algorithm on a mobile robot. The following image is an illustrative example of object recognition using the learned models:<br><br>						<center><a href="Images/RobotLearnAdapt/objRecogExample.jpg"><img src="Images/RobotLearnAdapt/Small/objRecogExample.jpg"alt="Using learned model for recognition"/></a></center> <br><br>						<b>Representative publications:</b><br>						<a href="http://www.cs.ttu.edu/~xiangli/">Xiang Li</a> and <a href="http://www.cs.ttu.edu/~smohan/">Mohan Sridharan</a>. <b>Move and the Robot will Learn: Vision-based Autonomous Learning of Object Models</b>. In the International Conference on Advanced Robotics (<a href="http://aamas2013.cs.umn.edu/">ICAR</a>), Montevideo, Uruguay, November 25-29, 2013.(<a href="./Papers/icar13_autoObjectLearn.pdf">pdf</a>)<br><br>						<a href="http://www.cs.ttu.edu/~xiangli/">Xiang Li</a>, <a href="http://www.cs.ttu.edu/~smohan">Mohan Sridharan</a> and <a href="http://www.cs.ttu.edu/~shiqizha">Shiqi Zhang</a>. <b>Autonomous Learning of Vision-based Layered Object Models on Mobile Robots</b>. In the International Conference on Robotics and Automation (<a href="http://www.icra2011.org">ICRA</a>), Shanghai, China, May 9-13, 2011. (<a href="./Papers/icra11_learnObjectModel.pdf">pdf</a>)						</li> <br><br>						<hr width="70%"\><br><br>						<li><a name="Multimodal"></a><b style="color:red"><u>Multimodal learning of object descriptions:</u></b> The objective is to enable robots to learn multimodal associative models of domain objects, using the resultant rich (object and domain) descriptions to pose specific high-level verbal queries to human participants. An overview of the multimodal learning approach is provided below in the context of a robot describing objects using learned visual and verbal <i>vocabularies</i> and associations between these vocabularies. <br><br>						<center><a href="Images/RobotLearnAdapt/multimodalOverview.jpg"><img src="Images/RobotLearnAdapt/Small/multimodalOverview.jpg"alt="Multimodal learning overview"/></a></center><br><br>						<b>Representative publication:</b><br>						 Kimia Salmani and <a href="http://www.cs.ttu.edu/~smohan/">Mohan Sridharan</a>. <b>Multi-Instance Active Learning with Online Labeling for Object Recognition</b>. In the 27th International Conference of the Florida AI Research Society (<a href="http://www.flairs-27.info/">FLAIRS</a>), Pensacola Beach, USA, May 21-23, 2014. (<a href="./Papers/flairs14_learnVisualVerbal.pdf">pdf</a>)<br><br>						 Ranjini Swaminathan and <a href="http://www.cs.ttu.edu/~smohan">Mohan Sridharan</a>. <b>Towards Robust Human-Robot Interaction using Multimodal Cues</b>. In the Human-Agent-Robot Teamwork Workshop (<a href="http://bradshawfamily.net/~samuel/zzz/hart2012/">HART</a>) at the International Conference on Human-Robot Interaction (<a href="http://hri2012.org/">HRI</a>), Boston, USA, March 5, 2012. (<a href="./Papers/hart12_visionSpeech.pdf">pdf</a>)						</li> <br><br>						<hr width="70%"\><br><br>						<li><a name="RL"></a><b style="color:red"><u>Augmented reinforcement learning:</u></b> The objective is to incrementally and autonomously merge the information extracted from high-level human feedback with the information extracted from sensory inputs, bootstrap off the two feedback mechanisms to make best use of the available information. A pictorial overview of the augmented reinforcement learning approach used to achieve this objective is given below. This approach has been evaluated in single and multiagent (simulated) game domains. <br><br>						<center><a href="Images/RobotLearnAdapt/arlOverview.jpg"><img src="Images/RobotLearnAdapt/Small/arlOverview.jpg"alt="Augmented reinforcement learning"/></a> </center> <br> <br>						<b>Representative publication:</b><br>						<a href="http://www.cs.ttu.edu/~smohan">Mohan Sridharan</a>. <b>Augmented Reinforcement Learning for Interaction with Non-Expert Humans in Agent Domains</b>. In the International Conference on Machine Learning Applications (<a href="http://www.icmla-conference.org/icmla11/">ICMLA</a>), Honolulu, Hawaii, December 18-21, 2011. (<a href="./Papers/icmla11_bootstrapRL.pdf">pdf</a>)						</li>					</ul> <br><br>					<hr/>					<h3><a name="ML projects"></a><b>Non-Robotics Projects:</b></h3> We also design learning and inference algorithms for challenges in critical (big data) application domains. <br>					<ul>						<li><a name="AgriApps"></a><b style="color:red"><u>Agricultural irrigation management and yield mapping:</u></b> Agricultural irrigation management poses tough challenges in arid and semi-arid regions, where crop water demand exceeds rainfall. Since daily grass or alfalfa reference ET values are widely used to estimate crop water demand, inaccurate reference ET estimates can impact irrigation costs and the demands on U.S. freshwater resources. ET networks calculate reference ET using accurate measurements of local meteorological data. With gaps in spatial coverage of existing agriculture-based ET networks (e.g., TXHPET) and lack of funding, there is an immediate need for alternative sources capable of filling data gaps without high maintenance and support costs. In collaboration with USDA-ARS and Texas A&M AgriLife Research, we adapt sophisticated machine learning algorithms that use weather observations from non-ET stations to accurately predict the reference ET values.<br><br>						<b>Representative publication:</b><br>						Daniel Holman, <a href="http://www.cs.ttu.edu/~smohan/">Mohan Sridharan</a>, <a href="http://www.ars.usda.gov/pandp/people/people.htm?personid=37079">Prasanna Gowda</a>, Dana Porter, Thomas Marek, Terry Howell and Jerry Moorhead. <b>Estimating Reference Evapotranspiration for Irrigation Scheduling in the Texas High Plains</b>. In the International Joint Conference on Artificial Intelligence (<a href="http://ijcai13.org/">IJCAI 2013</a>), Beijing, China, August 3-9, 2013. (<a href="./Papers/ijcai13_estimateET.pdf">pdf</a>)						</li> <br><br>						<li><a name="Climate"></a><b style="color:red"><u>Downscaling climate models:</u></b> Climate change and climate forecasts influence policies and planning in fields such as agriculture, ecological preservation and resource management. Although sophisticated global climate models can predict large scale weather patterns in grids of approx. 100km x 100km, they cannot make accurate regional weather predictions since they do not account for local geographic variations (within the grids) such as mountains and lakes. Obtaining high-resolution regional climate predictions by downscaling global models presents formidable big data challenges: (a) processes contributing to global models are highly non-linear; (b) global models and their relationships with regional observations are non-stationary; (c) sensitivity to initial conditions; and (d) use of Petabyte-scale historical data to learn models that can make predictions. In collaboration with the <a href="http://www.depts.ttu.edu/artsandsciences/csc/">Climate Science Center</a> at TTU and GFDL/Princeton, we are developing deep architectures to learn the relationships between global models and regional observations, thus making accurate regional predictions. <br> <br>						<b>Representative publication:</b><br>						Ranjini Swaminathan, <a href="http://www.cs.ttu.edu/~smohan">Mohan Sridharan</a> and <a href="http://www.katharinehayhoe.com/">Katharine Hayhoe</a>. <b>Convolutional Neural Networks for Climate Downscaling</b>. In the Climate Informatics Workshop (<a href="https://www2.image.ucar.edu/event/ci2012">CI 2012</a>), Boulder, USA, September 20-21, 2012. (<a href="./Papers/ci12_climateDownscale.pdf">pdf</a>)  (<a href="./Papers/ci12_CNNPoster.pdf">poster</a>)						</li>					</ul>  <br><br>					<hr/>					<h3><a name="Other videos"></a><b>Some cool videos:</b></h3>					<ul>						<li> A <a href="Images/RobotLearnAdapt/erratic_VerticalLaser.jpg">screenshot</a> of the 3D range map generated by a vertically-mounted laser on a mobile robot platform. <b>Click on the image to play the video of the robot mapping the entire lab</b>. You can look "into" the map approx. 90seconds from the start of the video. <br><br>						<center><a href="./Movies/InfoFuse/erratic_VerticalLaser.flv"><img src="Images/RobotLearnAdapt/Small/erratic_VerticalLaser.jpg"alt="Vertically-mounted laser map"/></a></center><br>						You can also look at a <a href="http://www.youtube.com/watch?v=8wL28W6xnEs&feature=feedu"><b>youtube version</b></a> of the video.						</li> <br>					</ul>					</div>				</tr>				</td>			</table>		</tr>		</td>	</table>	<table width=15% border=0 cellspacing=1 cellpadding=2>		<tr>		<td bgcolor=#000000>			<table width=100% border=0 cellspacing=0 cellpadding=10>				<tr>				<td bgcolor=#ffffff>				<font size=+1>					<center><b>JMP</center>					<small><br><br>					<a href="index.html">Home</a><br><br>					<a href="Research.html">Research</a>, <a href="SEARL.html">Lab</a><br><br>					<a href="Teaching.html">Teaching</a><br><br>					<a href="Outreach.html">Robot Outreach</a><br><br>					<a href="ResearchResume.pdf">CV</a>, <a href="Publications.html">Publications</a><br><br>				</tr>				</td>			</table>		</tr>		</td>	</table>	<br><!!--END WEBPAGE-->	</tr>	</td></table>